%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[a4paper, total={7.5in, 10.5in}]{geometry}
\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage[square,numbers]{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{float}
\usepackage{booktabs}  % for Pandas tables
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true
    }

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\setlength{\parskip}{0.3em}

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Markov Decision Processes \\ OMSCS 7641 - Machine Learning} % Title

\author{Kasey J. Evans -- kevans66} % Author name

\date{\today} % Date for the report

\begin{document}
\graphicspath{{../figures/}}

\maketitle % Insert the title, author and date

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1: Introduction
%----------------------------------------------------------------------------------------

In this report, we will explore three different algorithms for solving Markov Decision
Processes (MDPs): Value Iteration, Policy Iteration, and Q-Learning. We will explore
how these algorithms perform for two different MDPs: (1) navigating a maze with hazards
and (2) an implementation of the Texas Instruments calculator puzzle game, BlockDude. For
each of the two MDPs, we will explore two different sizes of state space. More details
will come in the subsequent section of the report.

%----------------------------------------------------------------------------------------
%	SECTION 2: MDP Descriptions
%----------------------------------------------------------------------------------------

\section{MDP Descriptions} \label{MDPS}

\textbf{Maze}

The Maze problem is a grid world problem where an agent must navigate a maze with
hazards to find a goal state. We will consider two mazes, one small and one large. The
mazes were generated using a random walk algorithm that creates winding corridors. The
start and goal states of the maze were selected such that the length of the shortest path
between the two states was maximal to ensure the MDPs were not trivial. The goal state
incurs a fairly large positive reward and each tile incurs a small negative reward to
encourage the agent to find the goal state. Hazards with a slightly larger negative
reward are randomly distributed throughout the maze. Finally, the agent has four move
actions available: up, right, down, and left. However, when the agent takes an action,
there is a 20\% chance that the agent will move in an unintended direction.

Maze is an \underline{interesting} problem is that it is close to the fundamental grid world problems
used to introduce reinforcement learning (RL): it will allow us to study the algorithms
in a simple framework without getting caught in problem details.

The facts of the Maze MDPs are summarized in table \ref{table:maze-facts} and visual
representations of the mazes can be found in figures \ref{fig:small-maze} and
\ref{fig:large-maze}. In the figures, a white 0 indicates a normal tile, a black 1
indicates an impenetrable walls, a red h indicates a hazard, the green s indicates the
start state, and the yellow g indicates the goal state.

\begin{table}
    \centering
    \caption{Maze MDP Facts}
    \label{table:maze-facts}
    \begin{tabular}{lr}
        \toprule
         Normal Tile Reward & -1 \\
         Hazard Reward & -3 \\
         Goal Reward & +50 \\
         Probability of Successful Action & 0.8 \\
         Small Maze Number of States & 231 \\
         Large Maze Number of States & 1,048 \\
        \bottomrule
        \end{tabular}
\end{table}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.2]{SmallMaze.png}
        \caption{Small Maze}
        \label{fig:small-maze}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.2]{LargeMaze.png}
        \caption{Large Maze}
        \label{fig:large-maze}
    \end{minipage}
\end{figure}

\textbf{BlockDude}

The BlockDude problem is an implementation of the Texas Instruments calculator puzzle
game by the same name. The objective is for an agent to reach a goal state. However, in
order for the agent to reach the goal state, it must pick up and move blocks from the
environment to build paths over obstacles (e.g. pits and hills in the landscape). Whereas
the maze problem is a top-down view, BlockDude is a side-on view. The agent has four
actions available: move east, move west, pick up block, place block. States are encoded
by the agent's position, facing direction (east or west), whether or not the agent is
holding a block, the position of each (interactive) block, and the positions of each
(non-interactive) block in the landscape. Because of this, the state space can quickly
grow to be quite large, posing a challenge to planning algorithms \cite{block_dude_desc}.
Even relatively simple problems have a large state space as we shall see.

The BlockDude problem is \underline{interesting} in that it allows us to formulate simple problems
in large state spaces, which should allow us to differentiate the behavior between the
model-based planning algorithms (VI \& PI) and the model-free learning algorithm
(Q-Learning): Because VI \& PI need to exhaustively analyze the entire state-space,
we should see them take a long time to solve even simple BlockDude problems that
Q-Learning might more easily solve. It is also interesting in contrast to the Maze
problem in that the agent is free to interactive with and change the environment, which
should add another layer of complexity to the problem.

The facts of the BlockDude MDPs are summarized in table \ref{table:bd-facts} and visual
representations of the mazes can be found in figures \ref{fig:small-bd} and
\ref{fig:large-bd}. In the figures, the white avatar indicates the agent, blocks with
brick-like patterns indicate non-interactive blocks, blocks with inscribed squares
indicate interactive blocks, and the door indicates the goal state. The agent has to
move the interactive blocks around to build stairs and climb over obstacles to reach
the goal state.

\begin{table}
    \centering
    \caption{BlockDude MDP Facts}
    \label{table:bd-facts}
    \begin{tabular}{lr}
        \toprule
         Action Reward & -1 \\
         Small BlockDude Number of States & 680 \\
         Large BlockDude Number of States & 14,200 \\
        \bottomrule
        \end{tabular}
\end{table}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{SmallBlockDude.png}
        \caption{Small BlockDude}
        \label{fig:small-bd}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{LargeBlockDude.png}
        \caption{Large BlockDude}
        \label{fig:large-bd}
    \end{minipage}
\end{figure}

\textbf{Comparison of MDPs}

There are similarities and differences between the two MDPs. Both MDPs are similar in that
they both require traversing obstacles to reach a terminating goal state -- not all MDPs
have terminating goal states. They are also similar in their reward functions: each
action in the environments incurs some small negative reward, and the goal state represents
the end of the negative rewards which drives the agent to the goal states. This property
also allows us to easily create a benchmark for the model performance by computing the
shortest path distance between the start and goal states using graph techniques.

In contrast, we see that BlockDude's state space grows very quickly:
Even relatively simple problems like the large problem (figure \ref{fig:large-bd}) have large
state spaces due to how states are encoded. Larger state spaces might help differentiate the
performance between the planning (VI \& PI) algorithms and the learning algorithm (RL). We
also see that the agent is able to interact with and change the environment in the
BlockDude problem. For these reasons, we might expect to see Q-Learning to perform
well relative to VI \& PI on BlockDude.


%----------------------------------------------------------------------------------------
%	SECTION 2: Value Iteration
%----------------------------------------------------------------------------------------

\section{Value Iteration} \label{VI}

Value Iteration (VI) is an algorithm for solving MDPs. Its goal is to estimate the utility of
each state in the MDP. It starts with an arbitrary value of utility at each state. Running those
values through the Bellman equation for utilities gives an updated, more accurate, utility
estimate of state utility. This update process is repeated until convergence is met: utility
changes at most very minimally. Once state utilities are estimated, a policy can be extracted
by selecting the action with the maximum expected utility (MEU) for each state. Because of this,
VI can only operate on problems with discrete domains that have known transition probabilities,
which is rather limiting.

In this implementation of VI, there is one main parameter for changing behavior, $\gamma$, which
affects the weight of distant states have on the utility of each state: the higher the value of
$\gamma$, the more valuable the rewards of future states. When $\gamma$ is 0, only immediate
rewards are considered. For this analysis, we will also explore how $\gamma$ affects the agent
performance.

\textbf{Maze}

Here we will analyze VI's performance on the Maze problem. For convergence, as specified by the
description of the algorithm, we will look for the change in utility to be small. Specifically,
here, we will look for the absolute difference in mean utility (meanV) to be less than 0.1
\footnote{\textbf{Note}: a table of all convergence criteria is available at the end of the report in table
\ref{table:convergence}}. The
choice of 0.1 is somewhat arbitrary. Typically, a good choice in convergence threshold is one
that allows the algorithm to quickly arrive at a good solution consistently. In the case of the
maze problem, we will see that 0.1 is an appropriate value, but other problems or algorithms
may need different thresholds.

Various metrics of VI's performance on this problem can be found in figures \ref{fig:small-maze-vi}
and \ref{fig:large-maze-vi} for the small and large variants of this problem, respectively. There
are two types of metrics presented: metrics collected during planning (meanV and PlanningTime),
and metrics collected during the execution of the agent on the problem (numActions and
CumulativeReward). The planning metrics help us determine convergence, and the execution metrics help
us determine the quality of the agent's performance. We need both types of metrics to get a full
picture. For example, an agent can converge quickly to a bad solution. \textbf{Note:} For low
values of iteration (before convergence is reached) execution metrics have extreme values. This is
because at those points, the agent has not yet learned how to traverse the maze and typically
runs around in loops for a very long time. For this reason, we trim the low iteration section of the
the plots illustrating the execution performance.

For this analysis, the metrics are aggregates (mean and standard deviation) of the results from
10 independent runs. However, the planning metrics do not change very much because the planning
phase of VI is deterministic. The execution phase is not deterministic because of the description
of the Maze problem: actions succeed with only 80\% probability.

A more detailed description of the metrics is as follows: 

\begin{enumerate}
    \item \textbf{numActions} (execution): the total number of actions taken by the agent at the
    given iteration while traversing the maze.
    \item \textbf{cumulativeReward} (execution): the sum total of reward as the agent traverses the
    maze from start to goal -- some paths might be longer (more actions) but incur fewer negative 
    rewards as the agent avoids hazards.
    \item \textbf{meanV} (planning): the mean utility of all states by iteration. When the derivative
    of this goes to 0, the algorithm has converged.
    \item \textbf{PlanningTime (ms)} (planning): the total time (in milliseconds) spent updating the
    utility matrix
\end{enumerate}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze6_gamma.png}
        \caption{Small Maze VI Performance by Gamma}
        \label{fig:small-maze-vi}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze9_gamma.png}
        \caption{Large Maze VI Performance by Gamma}
        \label{fig:large-maze-vi}
    \end{minipage}
\end{figure}

With the metrics being explained, we will now analyze our results.

When $\gamma$ is low (below 0.98 for the small problem, below 0.99 for the large problem), the agents
tend to get caught in loops and their execution metrics lose a lot of meaning (extremely high and volatile). In the
Maze problem, the start and end states are selected to maximize the shorted path distance in the environments.
Because of this, it should follow why low values of $\gamma$ result in agents getting caught in loops: the only
positive reward (or release from the low negative rewards from each action) is far-away. This is the
same reason why the lowest value of $\gamma$ needed to avoid getting caught in loops is higher for the
larger problem: the goal state is even farther away.

The effect of $\gamma$ is also evident by looking at the meanV plots: small reductions of $\gamma$ result
in large shifts in the meanV curves. Larger $\gamma$ will be more negative as future states (and their
negative rewards) have larger contribution at each iteration.

In general, we see larger values of $\gamma$ converge more quickly in the number of iterations for both problems --
the total planning time is insignificantly different. However, when we consider the numActions, we see the algorithm
converges to roughly the same number of actions and total reward, meaning the quality of their solutions are similar.
There is some slight variation in the exact number of actions, but this is most likely due to the stochastic nature
of the problem.

In conclusion, a good agent is one that converges quickly to a good solution with quick convergence being determined by
a short planning time and a good solution having high reward. Since we see different values of $\gamma$ each converging
to roughly the same number of actions and reward but higher values of $\gamma$ converging in slightly fewer iterations,
the best parameters for this problem are higher $\gamma$, specifically: $\gamma >= 0.99$ (small) and $\gamma >= 0.999$ (large).

\textbf{BlockDude}

The analysis of the BlockDude problem will be carried out in much the same way at the analysis of the Maze problem.
The software used to conduct the BlockDude problem analysis is slightly different from that used for the Maze. For
that reason, there are a few differences in the exact metrics used:

\begin{enumerate}
    \item maxV is used instead of meanV to identify and report convergence behavior, but this should be of little
    consequence as mean and max should both identify roughly the same iteration as the convergence point. However,
    they are at different scales which will impact the value of the convergence threshold.
    \item Reward and number of actions are only recorded for the final, converged iteration. We can still use
    these metrics to assess the quality of the agents, but we won't be able to plot them as a function of iteration.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[scale=0.43]{BlockDude_vi_gamma.png}
    \caption{BlockDude VI Performance by Gamma}
    \label{fig:bd-vi}
\end{figure}

The results of running VI on the BlockDude problem can be found in figure \ref{fig:bd-vi}. For convergence criterion,
here we use $\Delta maxV <= 0.001$ determined through experimentation using the reasoning explained above for Maze. Here, we
see generally that higher $\gamma$ results in longer wallclock time. As explained previously, higher values of $\gamma$
result in more weight for more distant states. It is possible that the implementation of VI used in this analysis terminates
early if the contribution of far away states is below some threshold. This is a possible explanation for the effect of
$\gamma$ on wallclock.

\begin{table}
    \centering
    \caption{BlockDude VI Performance at Convergence}
    \label{table:bd-vi-execution}
    \begin{tabular}{lllrrrr}
        \toprule
        Size & Gamma &  Total Iterations &  Cumulative Wallclock (ms) &  totalRewardAtConvergence &  numStepsAtConvergence \\
        \midrule
        Small &         0.7 &    20 &               658 &                       -19 &                     19 \\
        Small &         0.8 &    31 &               275 &                       -19 &                     19 \\
        Small &         0.9 &    66 &               586 &                       -19 &                     19 \\
        Small &        0.99 &   688 &              5,829 &                       -19 &                     19 \\
        \midrule
        Large &         0.7 &    20 &              6,336 &                    -1,249 &                   1,249 \\
        Large &         0.8 &    31 &              8,065 &                    -1,249 &                   1,249 \\
        Large &         0.9 &    66 &             15,204 &                       -94 &                     94 \\
        Large &        0.99 &   688 &            154,031 &                       -94 &                     94 \\
        \bottomrule
        \end{tabular}
    \end{table}

The execution metrics at the convergence point for BlockDude can be found in table \ref{table:bd-vi-execution}. We
see for the small BlockDude problem, although the agent when $\gamma = 0.7$ converges in the fewest number of iterations,
the agent when $\gamma = 0.8$ converges in the lowest wallclock time. All values of $\gamma$ converge to the optimal
number of steps. Therefore, the optimal value of $\gamma$ of the values explored is \textbf{0.8}.

The Large BlockDude problem obviously takes longer to converge than the small BlockDude problem even for small values of $\gamma$
due to the much larger number of states. Convergence time grows very quickly with $\gamma$. We see convergence to a
sub-optimal solution when $\gamma < 0.8$ and convergence to the optimal solution when $\gamma >= 0.8$. It is possible that the
agent is getting caught in some loop or otherwise does some sub-optimal sequence of actions resulting from only considering more
immediate rewards. Larger values of $\gamma$, as discussed previously, allow the agent to consider rewards from distant states.
This is particularly important for the large BlockDude problem which has the largest number of states in this analysis. There
are also no intermediate  (non-negative) rewards along the "path" to the solution, so the agent really needs a high $\gamma$ to
find the goal.

Because of this, for the large BlockDude problem, the optimal value of $\gamma$ of the values explored is \textbf{0.9}.

%----------------------------------------------------------------------------------------
%	SECTION 3: Policy Iteration
%----------------------------------------------------------------------------------------

\section{Policy Iteration} \label{PI}

Policy Iteration (PI) is another algorithm for solving MDPs. It shares some characteristics with VI.
PI first starts with an arbitrary policy for each state then carries out iteratively alternating between two phases:
policy evaluation and policy improvement. The first phase, policy evaluation, attempts to estimate the utility of each
state given the policy. In the implementation of PI explored in this analysis, the utility is estimated using VI.
Afterwards, a new policy is obtained by calculating the MEU policy given the new state utilities considering only the next step.
This process is repeated until the policy improvement stage no longer changes the policy, which we will identify here
by noting a small change in state utilities for consistency with VI. This has the benefit of allowing small changes in the policy
that may not have a significant impact on the solution.

In comparison to VI, PI tends to converge in fewer iterations. Further, in a variant of PI called "Modified Policy
Iteration" (where VI is constrained to only consider state-action pairs in the given policy), PI tends to be more efficient
than VI \cite{textbook}. Both VI and PI are subject to the same limitation that the algorithms can only be applied to
discrete problems with known transition probabilities. Finally, as with VI, the key parameter for controlling behavior
is $\gamma$, and so we will consider the performance as a function of $\gamma$ in this analysis.

\textbf{Maze}

The performance of PI on the large and small Maze problems can be seen in figures \ref{fig:small-maze-pi} and \ref{fig:large-maze-pi},
respectively. While we did not meet convergence for over 100 iterations with VI, with PI, convergence is met within about 6 and 10 iterations
for the small and large maze problems, respectively. These correspond to the points in the meanV plots where the value levels off. However,
each iteration of PI is much more expensive than VI's. This is possibly due to how -- in this implementation of PI -- each iteration of
PI requires running VI until convergence which can be many iterations. Typically, if the PI implementation uses the modified PI algorithm
described above, one would expect each iteration of PI's inner VI to be faster than each iteration of proper VI. In this case, BURLAP, we
see that the PI implemented references a policy-constrained VI. However, we still see consistently PI taking much longer than VI in terms of
time. It is possible that we are just unlucky here, and the cost of the multiple inner VI runs per iteration of PI outweighs the cost-savings
associated with the policy-constraint. However, it is worth noting that other students are also reporting on the class forums on Ed that they
observe PI to take longer than VI, and what we may be observing here is an inefficient implementation of PI.

We see a significant jump in planning time for both maze problems when $\gamma$ increases past 0.99. This is likely due to similar reasons
described in section \ref{VI} causing the inner VI loops to continue to iterate. Furthermore, when values of $\gamma$ below about 0.98 and 0.99
for the small and large variants, respectively, are explored, we again observe the same difficulty converging observed in section \ref{VI}.
Again for similar reasons, records from those runs are not shown (the agent would get caught in loops endlessly).

Finally, when we look at the number of actions and cumulative reward plots, we do not see very significant differences between values of
$\gamma$. Therefore, the optimal values of $\gamma$ for the small and large maze problems are \textbf{0.98 and 0.99}, respectively.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze6_gamma_pi.png}
        \caption{Small Maze PI Performance by Gamma}
        \label{fig:small-maze-pi}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze9_gamma_pi.png}
        \caption{Large Maze PI Performance by Gamma}
        \label{fig:large-maze-pi}
    \end{minipage}
\end{figure}

\textbf{BlockDude}

The PI performance metrics for BlockDude can be found in figure \ref{fig:bd-pi}, and the metrics at convergence can be
found in table \ref{table:bd-pi-execution}. As with the Maze problem, we again see PI converge in fewer iterations than VI (attention:
$\Delta maxV$ plot). The plateaus observed for $\gamma = 0.99$ are likely the result of the inner VI runs hitting the
iteration limit. However, even with the inner VI hitting the limit, we still see the algorithm able to converge to the
optimal solution. As before, we see convergence to a sub-optimal solution for the large problem when $\gamma < 0.9$. We
also see a significant jump in wallclock time for $\gamma > 0.9$. For these reasons, the optimal values for the small and
large BlockDude problems is \textbf{0.7 and 0.9}, respectively.

In comparison to VI, PI for this problem was able to converge to an equally performant solution but in about 10 times longer
time. So, VI is clearly superior. However, again, this could possibly be do implementation details in the BURLAP package, as
described earlier. It is possible that a different implementation of PI would prove to be superior to VI.

\begin{figure}
    \centering
    \includegraphics[scale=0.43]{BlockDude_pi_gamma.png}
    \caption{BlockDude PI Performance by Gamma}
    \label{fig:bd-pi}
\end{figure}

\begin{table}
    \centering
    \caption{BlockDude PI Performance at Convergence}
    \label{table:bd-pi-execution}
    \begin{tabular}{llrrrr}
        \toprule
        Size & Gamma &  Total Iterations &  Cumulative Wallclock (s) &  totalRewardAtConvergence &  numStepsAtConvergence \\
        \midrule
        Small &         0.7 &    14 &              1,368 &                       -19 &                     19 \\
        Small &         0.8 &    21 &              1,845 &                       -19 &                     19 \\
        Small &         0.9 &    20 &              3,052 &                       -19 &                     19 \\
        Small &        0.99 &    15 &              9,068 &                       -19 &                     19 \\
        \midrule
        Large &         0.7 &    14 &             26,425 &                    -1,249 &                   1,249 \\
        Large &         0.8 &    20 &             51,741 &                    -1,249 &                   1,249 \\
        Large &         0.9 &    29 &            148,630 &                       -94 &                     94 \\
        Large &        0.99 &    41 &            838,579 &                       -94 &                     94 \\
        \bottomrule
        \end{tabular}
    \end{table}

%----------------------------------------------------------------------------------------
%	SECTION 4: Q-Learning
%----------------------------------------------------------------------------------------

\section{Q-Learning} \label{QL}

The final algorithm for solving MDPs that we will analyze here is Q-Learning. Out of the algorithms explored in this analysis,
Q-Learning is the only model-free learning algorithm. This means that -- unlike VI \& PI -- Q-Learning is not limited to
discrete environments with known transition functions which makes it applicable to many more problems.

In this analysis, we will consider the epsilon greedy Q-Learning algorithm. This algorithm derives a policy from an estimate
of the action-state utility function, commonly referred to as Q in the literature. The policy derived is the argmax of Q. The Q
function is estimated through an iterative process: in a given state ($s$), execute some action ($a$), receive some reward ($r$),
observe the resulting state ($s'$), and update the $\hat{Q}(s,a)$ value (the estimated Q value) for the given state-action pair
according to:

\begin{equation*}
    \hat{Q}_{n}(s,a) \leftarrow (1 - \alpha_{n})\hat{Q}_{n-1}(s,a) + \alpha_{n}[r + \gamma\max_{a'} \hat{Q}_{n-1}(s', a')]
\end{equation*}

Where $\gamma$ is the discount factor described earlier, and $\alpha$ is a new parameter -- the learning rate -- that is slowly
decreased by iteration, $n$, discounting the effect of new information. $\alpha$ is helpful in obtaining convergence
\cite{ml_textbook}.

The epsilon in "epsilon-greedy Q-Learning" is the "explore-exploit" parameter. It introduces some randomness when picking the
action $a$ in the algorithm described above. With probability $1 - \epsilon$, the agent picks the action with maximal Q value.
When epsilon is 0, the agent only exploits its understanding of the problem by selecting the action it thinks is best.
In contrast, when epsilon is near 1, the agent has a high probability to select the next action randomly and thus explores.
Typically, a balance of exploration and exploitation are needed to learn how to solve a problem well.

We will explore the effects of all three parameters on the performance of Q-Learning on these two problems in the subsequent
sections.

\textbf{Gamma}

The impact of $\gamma$ on the performance of Q-Learning for the Maze and BlockDude problems can be found in figures
\ref{fig:maze-ql-gamma} and \ref{fig:bd-ql-gamma}, respectively. When $\gamma$ is low, for the Maze problem we see divergent
behavior: the number of steps starts small but begins grows as the number of episodes increases. As a reminder, numSteps is
not cumulative but instead shows the number of steps needed for the agent to solve the Maze problem at the given iteration.
We expect the agent to become "more intelligent" as episodes increases and thus we expect the number of steps to decrease.
It is possible that this behavior
is due to the learning rate $\alpha$, which decreases the influence of new information over time. Since $\gamma$ is low,
the agent does not "see" the far-away reward of the goal state. By stumbling around, the agent might eventually get into a
position where the influence of the goal state becomes significant. However, by that time, it is possible that the
value of $\alpha$ is too low for the agent to take that influence into consideration when updating its $\hat{Q}$ function.
The effect is larger in the large Maze problem than the small Maze problem where $\gamma < 0.9$ experience divergence.
This is consistent with the preceding argument because in the large Maze problem, the goal state is even further away.

Further, in the Maze problem, the effect of the far-off goal is exacerbated by the interspersed hazards, some of which block
paths to the goal. Upon inspection of some of the episode play-backs where we can watch a visual representation of the agent
solving the Maze step-by-step, it appears that early on, the agent does not quite learn the effect of the hazards and freely
steps onto them. However, after a few episodes, the agent appears to learn the effect of the hazards and learns that it should
avoid them. Since some of the hazards block the path to the goal, the agent needs to learn that some hazards are unavoidable.
When $\gamma$ is low, it appears the agent is not quite capable of learning that.

Moving on to the BlockDude problem, interestingly, for the small problem, we see little effect of $\gamma$ on the agent's
performance. We do see that higher values of $\gamma$ converge to the optimal solution more quickly, however, the effect is
not very  pronounced. This is likely due to the smaller state of the small BlockDude problem and we might see more pronounced
changes in performance at even lower values of $\gamma$. For the large problem, on the other hand, we do see a pronounced
impact of $\gamma$ where the agent is much more efficient at solving the problem when $\gamma$ is high. Still, the number
of iterations and the amount of time needed for the Q-Learning agent to solve the large BlockDude problem is worse than either VI
or PI even at the highest $\gamma$.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{Maze_Q_Learning_gamma.png}
        \caption{Maze Q-Learning Performance by Gamma}
        \label{fig:maze-ql-gamma}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{BlockDude_Q_Learning_Gamma_2.png}
        \caption{BlockDude Q-Learning Performance by Gamma}
        \label{fig:bd-ql-gamma}
    \end{minipage}
\end{figure}

\textbf{Epsilon}

$\epsilon$ is the "explore exploit" parameter. As discussed previously, the higher the value of $\epsilon$, the more often an agent
will randomly select its next move. This means that when $\epsilon$ is high, we should expect the agents to explore more of the environment and
tend to avoid getting stuck in local optima. However, when $\epsilon$ is too high, this can also have a negative effect as the agents may tend
to wander around avoiding the correct path. This is especially important for our problems because there are slight negative rewards for each
action taken, so the agent is best to quickly reach the goal.

For the Maze problem, we see some evidence of the deleterious effect of high $\epsilon$ in figure \ref{fig:maze-ql-epsilon}:
average reward is lower and number of steps is higher for higher values of $\epsilon$. The results shown in figure \ref{fig:maze-ql-epsilon}
are from experiments with $\gamma$ fixed at 0.9. It is possible that the divergent behavior observed at low values of $\gamma$ discussed in
the previous section could be offset by higher $\epsilon$. In the previous section, we observed what appeared to be the agent learning enough to
avoid the path-blocking hazards and getting stuck in a loop, a local optima. Higher $\epsilon$ values might cause the agent to randomly move
over the obstacles and move toward the goal.

For the BlockDude problem, we surprisingly see very little effect of $\epsilon$ on the agent's performance. We would expect to see
a correlation between higher values of $\epsilon$ and lower reward, as observed for the Maze problem, due to the presence of a small
negative reward given to the agent at each action.


\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{Maze_Q_Learning_epsilon.png}
        \caption{Maze Q-Learning Performance by Epsilon}
        \label{fig:maze-ql-epsilon}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{BlockDude_Q_Learning_epsilon_2.png}
        \caption{BlockDude Q-Learning Performance by Epsilon}
        \label{fig:bd-ql-epsilon}
    \end{minipage}
\end{figure}

\textbf{Alpha}

Finally, we come to $\alpha$, the learning rate parameter. As discussed previously, when $\alpha$ is high, new information
is given more weight when updating the $\hat{Q}$ function. $\alpha$ is slowly lowered over time which has the effect that
the agent will start off exploring and learning then slowly start to rely more and more on what is has learned in the past.
This is helpful for convergence and means in later iterations, the agent will more directly attempt to solve the MDP rather
than learn and explore it. In terms of expected behavior, when $\alpha$ is too low, we should expect to see the agent
not exploring its environment and getting stuck in local optima more often. On the other hand, when $\alpha$ is too high, we
should expect to see more meandering of the agent and thus lower rewards and higher step counts.

The effect of $\alpha$ on the Q-Learning agent's performance on the Maze and BlockDude problems can be seen in figures
\ref{fig:maze-ql-alpha} and \ref{fig:bd-ql-alpha}, respectively. For the small Maze problem, we still little effect of alpha.
This could be due to the relatively small state space in this problem: the agent is able to learn quickly what it needs to do.
For the large Maze problem, we start to see some slight effect of $\alpha$. At higher values of $\alpha$, we see lower rewards
as expected due to the meandering nature high $\alpha$ brings. Still, the effect is slight. It is possible that larger ranges of
$\alpha$ would yield more differentiated behavior.

For the BlockDude problem, we see a strong effect of $\alpha$. This problem really benefits from high $\alpha$: higher $\alpha$
results in much faster convergence to better solutions to the problem (fewer steps). The effect is similar for both sizes.
This could again be due to the nature of the problem where each step incurs some small negative penalty and the only non-negative
reward comes from a far-away goal state. The path to the solution is complex and requires the agent to interact with its environment
to create a way to the solution. In this case, the solution to the BlockDude problem is more complex and it is likely for that
reason why we see such a strong effect of $\alpha$.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{Maze_Q_Learning_alpha.png}
        \caption{Maze Q-Learning Performance by Alpha}
        \label{fig:maze-ql-alpha}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{BlockDude_Q_Learning_alpha.png}
        \caption{BlockDude Q-Learning Performance by Alpha}
        \label{fig:bd-ql-alpha}
    \end{minipage}
\end{figure}

\begin{table}
    \centering
    \caption{Summary of Q-Learning Performance Metrics at Convergence with Tuned Parameters}
    \label{table:large-bd-ql}
    \begin{tabular}{llrrrrrr}
        \toprule
           &     & \multicolumn{2}{l}{numSteps} & \multicolumn{2}{l}{Cumulative Wallclock (s)} & \multicolumn{2}{l}{Iterations} \\
           &     &   Median &  IQR &           Median &    IQR &    Median &   IQR \\
        Problem & Size &          &      &                  &        &           &       \\
        \midrule
        BlockDude & Small &    19.00 & 0.00 &         0.37 &  0.04  &     58.00 &  2.00 \\
        BlockDude & Large &    94.00 & 0.00 &         57.34 &  0.83 &  4,571.00 &  3.00 \\
        \bottomrule
        \end{tabular}               
    \end{table} 

%----------------------------------------------------------------------------------------
%	Conclusion
%----------------------------------------------------------------------------------------

\section{Conclusion}

A summary of the performance of each algorithm on each problem and size can be found in table \ref{table:comparison}.
A good algorithm here is one that can quickly produce high quality (low number of steps) solutions to the problems.
Given these criteria, what we see is that VI clearly outperforms all the other algorithms for these problems. Although 
we see VI and PI both converge to high quality solutions, unfortunately we see PI take about an order of magnitude longer
in planning time. In theory, PI should generally perform better than VI. However, as explained above in section
\ref{PI}, it is possible that our implementation of PI (BURLAP) is inefficient as other students are reporting similar
performance issues for different problems.

Perhaps disappointingly, Q-Learning performed by-far the worst. For the Maze problem, Q-Learning was not able to
converge to a good solution and mostly just ran around the maze aimlessly, never truly learning how to traverse the Maze.
Q-Learning still performed poorly for the BlockDude problem, although it is possible to converge to the optimal solution
given a large amount of iterations. It is possible that a primary driver of Q-Learning's poor performance on these problems
is the lack of intermediate rewards. The interspersed path-blocking hazards in the Maze problem possibly caused the agent to
never converge.

In the end, we find the simplest algorithm, VI, to perform the best across the board. It is possible that this is an artifact
of PI's implementation that PI would have performed best with a more efficient implementation. By propagating the rewards from
the goal state to the start state, VI and PI are able to overcome the difficulties imposed by the lack of intermediate rewards 
and path-blocking hazards. To improve the performance of Q-Learning, intermediate rewards should be placed along the solution
path in the problems and the hazards in the maze problem should be adjusted to avoid blocking paths.

Finally, it is worth mentioning that although Q-Learning performed the worst, it does have a significant advantage in that
it can be applied to many more problems. VI and PI are limited in that they can only be applied to problems with discrete
environments and with known transition probabilities. Q-Learning, on the other hand, does not depend on these characteristics
and is therefore much more general of an algorithm: Q-Learning might take a while to solve a problem, but at least it can
solve it.

\begin{table}
    \centering
    \caption{Comparison of Optimal Runs By Algorithm, Problem, and Size}
    \label{table:comparison}
    \begin{tabular}{llllrrr}
        \toprule
        Algorithm & Problem &  Size &  Parameters & Num Steps & Cumulative Wallclock (ms) & Iterations \\
        \midrule
        VI & Maze & Small & $\gamma = 0.99$ & 165 & 400 & 160 \\
        VI & Maze & Large & $\gamma = 0.999$ & 170 & 2,000 & 175 \\
        VI & BlockDude & Small & $\gamma = 0.8$ & 19 & 275 & 31 \\
        VI & BlockDude & Large & $\gamma = 0.9$ & 94 & 15,200 & 66 \\
        \midrule
        PI & Maze & Small & $\gamma = 0.98$ & 165 & 3,700 & 4 \\
        PI & Maze & Large & $\gamma = 0.99$ & 165 & 90,000 & 10 \\
        PI & BlockDude & Small & $\gamma = 0.7$ & 19 & 1,400 & 14 \\
        PI & BlockDude & Large & $\gamma = 0.9$ & 94 & 150,000 & 29 \\
        \midrule
        Q-Learning & Maze & Small & $\alpha = 0.9$, $\gamma = 0.9$, $\epsilon = 0.7$ & 105,000 &  & 26 \\
        Q-Learning & Maze & Large & $\alpha = 0.9$, $\gamma = 0.9$, $\epsilon = 0.7$ & 1,600,000 &  & 71 \\
        Q-Learning & BlockDude & Small & $\alpha = 0.9$, $\gamma = 0.99$, $\epsilon = 0.9$ & 19 & 370,000 & 14 \\
        Q-Learning & BlockDude & Large & $\alpha = 0.9$, $\gamma = 0.99$, $\epsilon = 0.9$ & 94 & 57,000,000 & 29 \\
        \bottomrule
        \end{tabular}
    \end{table}

    \begin{table}
        \centering
        \caption{Convergence Criteria}
        \label{table:convergence}
        \begin{tabular}{llrl}
            \toprule
            Algorithm & Problem &  Threshold & Metric \\
            \midrule
            VI & Maze & 0.1 & $\Delta$ meanV \\
            VI & BlockDude & 0.001 & $\Delta \max{V}$ \\
            PI & Maze & 1 & $\Delta$ meanV \\
            PI & BlockDude & 0.001 & $\Delta \max{V}$ \\
            Q-Learning & Maze & 0.0001 & $\Delta$ meanReward \\
            Q-Learning & BlockDude & 0.0001 & $\Delta \max{Q}$ \\
            \bottomrule
            \end{tabular}
        \end{table}
    


%----------------------------------------------------------------------------------------
%	Bibliography
%----------------------------------------------------------------------------------------

    \bibliography{refs}{}
\bibliographystyle{plain}

\end{document}