%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[a4paper, total={7.5in, 10.5in}]{geometry}
\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage[square,numbers]{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{float}
\usepackage{booktabs}  % for Pandas tables
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true
    }

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\setlength{\parskip}{0.3em}

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Markov Decision Processes \\ OMSCS 7641 - Machine Learning} % Title

\author{Kasey J. Evans -- kevans66} % Author name

\date{\today} % Date for the report

\begin{document}
\graphicspath{{../figures/}}

\maketitle % Insert the title, author and date

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1: Introduction
%----------------------------------------------------------------------------------------

In this report, we will explore three different algorithms for solving Markov Decision
Processes (MDPs): Value Iteration, Policy Iteration, and Q-Learning. We will explore
how these algorithms perform for two different MDPs: (1) navigating a maze with hazards
and (2) an implementation of the Texas Instruments calculator puzzle game, BlockDude. For
each of the two MDPs, we will explore two different sizes of state space. More details
will come in the subsequent section of the report.

%----------------------------------------------------------------------------------------
%	SECTION 2: MDP Descriptions
%----------------------------------------------------------------------------------------

\section{MDP Descriptions} \label{MDPS}

\textbf{Maze}

The Maze problem is a grid world problem where an agent must navigate a maze with
hazards to find a goal state. We will consider two mazes, one small and one large. The
mazes were generated using a random walk algorithm that creates winding corridors. The
start and goal states of the maze were selected such that the length of the shortest path
between the two states was maximal to ensure the MDPs were not trivial. The goal state
incurs a fairly large positive reward and each tile incurs a small negative reward to
encourage the agent to find the goal state. Hazards with a slightly larger negative
reward are randomly distributed throughout the maze. Finally, the agent has four move
actions available: up, right, down, and left. However, when the agent takes an action,
there is a 20\% chance that the agent will move in an unintended direction.

Maze is an \underline{interesting} problem is that it is close to the fundamental grid world problems
used to introduce reinforcement learning (RL): it will allow us to study the algorithms
in a simple framework without getting caught in problem details.

The facts of the Maze MDPs are summarized in table \ref{table:maze-facts} and visual
representations of the mazes can be found in figures \ref{fig:small-maze} and
\ref{fig:large-maze}. In the figures, a white 0 indicates a normal tile, a black 1
indicates an impenetrable walls, a red h indicates a hazard, the green s indicates the
start state, and the yellow g indicates the goal state.

\begin{table}
    \centering
    \caption{Maze MDP Facts}
    \label{table:maze-facts}
    \begin{tabular}{lr}
        \toprule
         Normal Tile Reward & -1 \\
         Hazard Reward & -3 \\
         Goal Reward & +50 \\
         Probability of Successful Action & 0.8 \\
         Small Maze Number of States & 231 \\
         Large Maze Number of States & 1,048 \\
        \bottomrule
        \end{tabular}
\end{table}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.2]{SmallMaze.png}
        \caption{Small Maze}
        \label{fig:small-maze}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.2]{LargeMaze.png}
        \caption{Large Maze}
        \label{fig:large-maze}
    \end{minipage}
\end{figure}

\textbf{BlockDude}

The BlockDude problem is an implementation of the Texas Instruments calculator puzzle
game by the same name. The objective is for an agent to reach a goal state. However, in
order for the agent to reach the goal state, it must pick up and move blocks from the
environment to build paths over obstacles (e.g. pits and hills in the landscape). Whereas
the maze problem is a top-down view, BlockDude is a side-on view. The agent has four
actions available: move east, move west, pick up block, place block. States are encoded
by the agent's position, facing direction (east or west), whether or not the agent is
holding a block, the position of each (interactive) block, and the positions of each
(non-interactive) block in the landscape. Because of this, the state space can quickly
grow to be quite large, posing a challenge to planning algorithms \cite{block_dude_desc}.
Even relatively simple problems have a large state space as we shall see.

The BlockDude problem is \underline{interesting} in that it allows us to formulate simple problems
in large state spaces, which should allow us to differentiate the behavior between the
model-based planning algorithms (VI \& PI) and the model-free learning algorithm
(Q-Learning): Because VI \& PI need to exhaustively analyze the entire state-space,
we should see them take a long time to solve even simple BlockDude problems that
Q-Learning might more easily solve. It is also interesting in contrast to the Maze
problem in that the agent is free to interactive with and change the environment, which
should add another layer of complexity to the problem.

The facts of the BlockDude MDPs are summarized in table \ref{table:bd-facts} and visual
representations of the mazes can be found in figures \ref{fig:small-bd} and
\ref{fig:large-bd}. In the figures, the white avatar indicates the agent, blocks with
brick-like patterns indicate non-interactive blocks, blocks with inscribed squares
indicate interactive blocks, and the door indicates the goal state. The agent has to
move the interactive blocks around to build stairs and climb over obstacles to reach
the goal state.

\begin{table}
    \centering
    \caption{BlockDude MDP Facts}
    \label{table:bd-facts}
    \begin{tabular}{lr}
        \toprule
         Action Reward & -1 \\
         Small BlockDude Number of States & 680 \\
         Large BlockDude Number of States & 14,200 \\
        \bottomrule
        \end{tabular}
\end{table}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{SmallBlockDude.png}
        \caption{Small BlockDude}
        \label{fig:small-bd}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{LargeBlockDude.png}
        \caption{Large BlockDude}
        \label{fig:large-bd}
    \end{minipage}
\end{figure}

\textbf{Comparison of MDPs}

There are similarities and differences between the two MDPs. Both MDPs are similar in that
they both require traversing obstacles to reach a terminating goal state -- not all MDPs
have terminating goal states. They are also similar in their reward functions: each
action in the environments incurs some small negative reward, and the goal state represents
the end of the negative rewards which drives the agent to the goal states. This property
also allows us to easily create a benchmark for the model performance by computing the
shortest path distance between the start and goal states using graph techniques.

In contrast, we see that BlockDude's state space grows very quickly:
Even relatively simple problems like the large problem (figure \ref{fig:large-bd}) have large
state spaces due to how states are encoded. Larger state spaces might help differentiate the
performance between the planning (VI \& PI) algorithms and the learning algorithm (RL). We
also see that the agent is able to interact with and change the environment in the
BlockDude problem. For these reasons, we might expect to see Q-Learning to perform
well relative to VI \& PI on BlockDude.


%----------------------------------------------------------------------------------------
%	SECTION 2: Value Iteration
%----------------------------------------------------------------------------------------

\section{Value Iteration} \label{VI}

Value Iteration (VI) is an algorithm for solving MDPs. Its goal is to estimate the utility of
each state in the MDP. It starts with an arbitrary value of utility at each state. Running those
values through the Bellman equation for utilities gives an updated, more accurate, utility
estimate of state utility. This update process is repeated until convergence is met: utility
changes at most very minimally. Once state utilities are estimated, a policy can be extracted
by selecting the action with the maximum expected utility (MEU) for each state. Because of this,
VI can only operate on problems with discrete domains that have known transition probabilities,
which is rather limiting.

In this implementation of VI, there is one main parameter for changing behavior, $\gamma$, which
affects the weight of distant states have on the utility of each state: the higher the value of
$\gamma$, the more valuable the rewards of future states. When $\gamma$ is 0, only immediate
rewards are considered. For this analysis, we will also explore how $\gamma$ affects the agent
performance.

\textbf{Maze}

Here we will analyze VI's performance on the Maze problem. For convergence, as specified by the
description of the algorithm, we will look for the change in utility to be small. Specifically,
here, we will look for the absolute difference in mean utility (meanV) to be less than 0.1. The
choice of 0.1 is somewhat arbitrary. Typically, a good choice in convergence threshold is one
that allows the algorithm to quickly arrive at a good solution consistently. In the case of the
maze problem, we will see that 0.1 is an appropriate value, but other problems or algorithms
may need different thresholds.

Various metrics of VI's performance on this problem can be found in figures \ref{fig:small-maze-vi}
and \ref{fig:large-maze-vi} for the small and large variants of this problem, respectively. There
are two types of metrics presented: metrics collected during planning (meanV and PlanningTime),
and metrics collected during the execution of the agent on the problem (numActions and
CumulativeReward). The planning metrics help us determine convergence, and the execution metrics help
us determine the quality of the agent's performance. We need both types of metrics to get a full
picture. For example, an agent can converge quickly to a bad solution. \textbf{Note:} For low
values of iteration (before convergence is reached) execution metrics have extreme values. This is
because at those points, the agent has not yet learned how to traverse the maze and typically
runs around in loops for a very long time. For this reason, we trim the low iteration section of the
the plots illustrating the execution performance.

For this analysis, the metrics are aggregates (mean and standard deviation) of the results from
10 independent runs. However, the planning metrics do not change very much because the planning
phase of VI is deterministic. The execution phase is not deterministic because of the description
of the Maze problem: actions succeed with only 80\% probability.

A more detailed description of the metrics is as follows: 

\begin{enumerate}
    \item \textbf{numActions} (execution): the total number of actions taken by the agent at the
    given iteration while traversing the maze.
    \item \textbf{cumulativeReward} (execution): the sum total of reward as the agent traverses the
    maze from start to goal -- some paths might be longer (more actions) but incur fewer negative 
    rewards as the agent avoids hazards.
    \item \textbf{meanV} (planning): the mean utility of all states by iteration. When the derivative
    of this goes to 0, the algorithm has converged.
    \item \textbf{PlanningTime (ms)} (planning): the total time (in milliseconds) spent updating the
    utility matrix
\end{enumerate}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze6_gamma.png}
        \caption{Small Maze VI Performance by Gamma}
        \label{fig:small-maze-vi}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze9_gamma.png}
        \caption{Large Maze VI Performance by Gamma}
        \label{fig:large-maze-vi}
    \end{minipage}
\end{figure}

With the metrics being explained, we will now analyze our results.

When $\gamma$ is low (below 0.98 for the small problem, below 0.99 for the large problem), the agents
tend to get caught in loops and their execution metrics lose a lot of meaning (extremely high and volatile). In the
Maze problem, the start and end states are selected to maximize the shorted path distance in the environments.
Because of this, it should follow why low values of $\gamma$ result in agents getting caught in loops: the only
positive reward (or release from the low negative rewards from each action) is far-away. This is the
same reason why the lowest value of $\gamma$ needed to avoid getting caught in loops is higher for the
larger problem: the goal state is even farther away.

The effect of $\gamma$ is also evident by looking at the meanV plots: small reductions of $\gamma$ result
in large shifts in the meanV curves. Larger $\gamma$ will be more negative as future states (and their
negative rewards) have larger contribution at each iteration.

In general, we see larger values of $\gamma$ converge more quickly in the number of iterations for both problems --
the total planning time is insignificantly different. However, when we consider the numActions, we see the algorithm
converges to roughly the same number of actions and total reward, meaning the quality of their solutions are similar.
There is some slight variation in the exact number of actions, but this is most likely due to the stochastic nature
of the problem.

In conclusion, a good agent is one that converges quickly to a good solution with quick convergence being determined by
a short planning time and a good solution having high reward. Since we see different values of $\gamma$ each converging
to roughly the same number of actions and reward but higher values of $\gamma$ converging in slightly fewer iterations,
the best parameters for this problem are higher $\gamma$, specifically: $\gamma >= 0.99$ (small) and $\gamma >= 0.999$ (large).

\textbf{BlockDude}

The analysis of the BlockDude problem will be carried out in much the same way at the analysis of the Maze problem.
The software used to conduct the BlockDude problem analysis is slightly different from that used for the Maze. For
that reason, there are a few differences in the exact metrics used:

\begin{enumerate}
    \item maxV is used instead of meanV to identify and report convergence behavior, but this should be of little
    consequence as mean and max should both identify roughly the same iteration as the convergence point. However,
    they are at different scales which will impact the value of the convergence threshold.
    \item Reward and number of actions are only recorded for the final, converged iteration. We can still use
    these metrics to assess the quality of the agents, but we won't be able to plot them as a function of iteration.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[scale=0.43]{BlockDude_vi_gamma.png}
    \caption{BlockDude VI Performance by Gamma}
    \label{fig:bd-vi}
\end{figure}

The results of running VI on the BlockDude problem can be found in figure \ref{fig:bd-vi}. For convergence criterion,
here we use $\Delta maxV <= 0.001$ determined through experimentation using the reasoning explained above for Maze. Here, we
see generally that higher $\gamma$ results in longer wallclock time. As explained previously, higher values of $\gamma$
result in more weight for more distant states. It is possible that the implementation of VI used in this analysis terminates
early if the contribution of far away states is below some threshold. This is a possible explanation for the effect of
$\gamma$ on wallclock.

\begin{table}
    \centering
    \caption{BlockDude VI Performance at Convergence}
    \label{table:bd-vi-execution}
    \begin{tabular}{lllrrrr}
        \toprule
        Size & Gamma &  Total Iterations &  Cumulative Wallclock (s) &  totalRewardAtConvergence &  numStepsAtConvergence \\
        \midrule
        Small &         0.7 &    20 &               658 &                       -19 &                     19 \\
        Small &         0.8 &    31 &               275 &                       -19 &                     19 \\
        Small &         0.9 &    66 &               586 &                       -19 &                     19 \\
        Small &        0.99 &   688 &              5,829 &                       -19 &                     19 \\
        \midrule
        Large &         0.7 &    20 &              6,336 &                    -1,249 &                   1,249 \\
        Large &         0.8 &    31 &              8,065 &                    -1,249 &                   1,249 \\
        Large &         0.9 &    66 &             15,204 &                       -94 &                     94 \\
        Large &        0.99 &   688 &            154,031 &                       -94 &                     94 \\
        \bottomrule
        \end{tabular}
    \end{table}

The execution metrics at the convergence point for BlockDude can be found in table \ref{table:bd-vi-execution}. We
see for the small BlockDude problem, although the agent when $\gamma = 0.7$ converges in the fewest number of iterations,
the agent when $\gamma = 0.8$ converges in the lowest wallclock time. All values of $\gamma$ converge to the optimal
number of steps. Therefore, the optimal value of $\gamma$ of the values explored is \textbf{0.8}.

The Large BlockDude problem obviously takes longer to converge than the small BlockDude problem even for small values of $\gamma$
due to the much larger number of states. Convergence time grows very quickly with $\gamma$. We see convergence to a
sub-optimal solution when $\gamma < 0.8$ and convergence to the optimal solution when $\gamma >= 0.8$. It is possible that the
agent is getting caught in some loop or otherwise does some sub-optimal sequence of actions resulting from only considering more
immediate rewards. Larger values of $\gamma$, as discussed previously, allow the agent to consider rewards from distant states.
This is particularly important for the large BlockDude problem which has the largest number of states in this analysis. There
are also no intermediate  (non-negative) rewards along the "path" to the solution, so the agent really needs a high $\gamma$ to
find the goal.

Because of this, for the large BlockDude problem, the optimal value of $\gamma$ of the values explored is \textbf{0.9}.

%----------------------------------------------------------------------------------------
%	SECTION 3: Policy Iteration
%----------------------------------------------------------------------------------------

\section{Policy Iteration} \label{PI}

Policy Iteration (PI) is another algorithm for solving MDPs. It shares some characteristics with VI.
PI first starts with an arbitrary policy for each state then carries out iteratively alternating between two phases:
policy evaluation and policy improvement. The first phase, policy evaluation, attempts to estimate the utility of each
state given the policy. In the implementation of PI explored in this analysis, the utility is estimated using VI.
Afterwards, a new policy is obtained by calculating the MEU policy given the new state utilities considering only the next step.
This process is repeated until the policy improvement stage no longer changes the policy, which we will identify here
by noting a small change in state utilities for consistency with VI. This has the benefit of allowing small changes in the policy
that may not have a significant impact on the solution.

In comparison to VI, PI tends to converge in fewer iterations. Further, in a variant of PI called "Modified Policy
Iteration" (where VI is constrained to only consider state-action pairs in the given policy), PI tends to be more efficient
than VI \cite{textbook}. Both VI and PI are subject to the same limitation that the algorithms can only be applied to
discrete problems with known transition probabilities. Finally, as with VI, the key parameter for controlling behavior
is $\gamma$, and so we will consider the performance as a function of $\gamma$ in this analysis.

\textbf{Maze}

The performance of PI on the large and small Maze problems can be seen in figures \ref{fig:small-maze-pi} and \ref{fig:large-maze-pi},
respectively. While we did not meet convergence for over 100 iterations with VI, with PI, convergence is met within about 6 and 10 iterations
for the small and large maze problems, respectively. These correspond to the points in the meanV plots where the value levels off. However,
each iteration of PI is much more expensive than VI's. This is possibly due to how -- in this implementation of PI -- each iteration of
PI requires running VI until convergence which can be many iterations. Typically, if the PI implementation uses the modified PI algorithm
described above, one would expect each iteration of PI's inner VI to be faster than each iteration of proper VI. In this case, BURLAP, we
see that the PI implemented references a policy-constrained VI. However, we still see consistently PI taking much longer than VI in terms of
time. It is possible that we are just unlucky here, and the cost of the multiple inner VI runs per iteration of PI outweighs the cost-savings
associated with the policy-constraint. However, it is worth noting that other students are also reporting on the class forums on Ed that they
observe PI to take longer than VI, and what we may be observing here is an inefficient implementation of PI.

We see a significant jump in planning time for both maze problems when $\gamma$ increases past 0.99. This is likely due to similar reasons
described in section \ref{VI} causing the inner VI loops to continue to iterate. Furthermore, when values of $\gamma$ below about 0.98 and 0.99
for the small and large variants, respectively, are explored, we again observe the same difficulty converging observed in section \ref{VI}.
Again for similar reasons, records from those runs are not shown (the agent would get caught in loops endlessly).

Finally, when we look at the number of actions and cumulative reward plots, we do not see very significant differences between values of
$\gamma$. Therefore, the optimal values of $\gamma$ for the small and large maze problems are \textbf{0.98 and 0.99}, respectively.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze6_gamma_pi.png}
        \caption{Small Maze PI Performance by Gamma}
        \label{fig:small-maze-pi}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{maze9_gamma_pi.png}
        \caption{Large Maze PI Performance by Gamma}
        \label{fig:large-maze-pi}
    \end{minipage}
\end{figure}

\textbf{BlockDude}

\begin{figure}
    \centering
    \label{fig:DT-ccp}
    \includegraphics[scale=0.43]{BlockDude_pi_gamma.png}
    \caption{BlockDude PI Performance by Gamma}
\end{figure}


\begin{table}
    \centering
    \caption{BlockDude PI Performance at Convergence}
    \begin{tabular}{llrrrr}
        \toprule
        Size & Gamma &  Total Iterations &  Cumulative Wallclock (s) &  totalRewardAtConvergence &  numStepsAtConvergence \\
        \midrule
        Small &         0.7 &    14 &              1,368 &                       -19 &                     19 \\
        Small &         0.8 &    21 &              1,845 &                       -19 &                     19 \\
        Small &         0.9 &    20 &              3,052 &                       -19 &                     19 \\
        Small &        0.99 &    15 &              9,068 &                       -19 &                     19 \\
        \midrule
        Large &         0.7 &    14 &             26,425 &                    -1,249 &                   1,249 \\
        Large &         0.8 &    20 &             51,741 &                    -1,249 &                   1,249 \\
        Large &         0.9 &    29 &            148,630 &                       -94 &                     94 \\
        Large &        0.99 &    41 &            838,579 &                       -94 &                     94 \\
        \bottomrule
        \end{tabular}
    \end{table}

%----------------------------------------------------------------------------------------
%	SECTION 4: Q-Learning
%----------------------------------------------------------------------------------------

\section{Q-Learning} \label{QL}

\textbf{Maze}

\textbf{BlockDude}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{Maze_Q_Learning_gamma.png}
        \caption{Maze Q-Learning Performance by Gamma}
        \label{fig:maze-ql-gamma}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{BlockDude_Q_Learning_Gamma_2.png}
        \caption{BlockDude Q-Learning Performance by Gamma}
        \label{fig:bd-ql-gamma}
    \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{Maze_Q_Learning_epsilon.png}
        \caption{Maze Q-Learning Performance by Epsilon}
        \label{fig:maze-ql-epsilon}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{BlockDude_Q_Learning_epsilon_2.png}
        \caption{BlockDude Q-Learning Performance by Epsilon}
        \label{fig:bd-ql-epsilon}
    \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{Maze_Q_Learning_alpha.png}
        \caption{Maze Q-Learning Performance by Alpha}
        \label{fig:maze-ql-alpha}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.4]{BlockDude_Q_Learning_alpha.png}
        \caption{BlockDude Q-Learning Performance by Alpha}
        \label{fig:bd-ql-alpha}
    \end{minipage}
\end{figure}

\begin{table}
    \centering
    \caption{Summary of Q-Learning Performance Metrics at Convergence with Tuned Parameters}
    \label{table:large-bd-ql}
    \begin{tabular}{llrrrrrr}
        \toprule
           &     & \multicolumn{2}{l}{numSteps} & \multicolumn{2}{l}{Cumulative Wallclock (s)} & \multicolumn{2}{l}{Iterations} \\
           &     &   Median &  IQR &           Median &    IQR &    Median &   IQR \\
        Problem & Size &          &      &                  &        &           &       \\
        \midrule
        BlockDude & Small &    19.00 & 0.00 &         0.37 &  0.04  &     58.00 &  2.00 \\
        BlockDude & Large &    94.00 & 0.00 &         57.34 &  0.83 &  4,571.00 &  3.00 \\
        \bottomrule
        \end{tabular}               
    \end{table} 
    

%----------------------------------------------------------------------------------------
%	Bibliography
%----------------------------------------------------------------------------------------

    \bibliography{refs}{}
\bibliographystyle{plain}

\end{document}